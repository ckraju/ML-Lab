{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckraju/ML-Lab/blob/master/question_answering_ssai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KImyTN6qtfZ"
      },
      "source": [
        "# NLP Workshop : Passage Retrieval and Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRBzKvcFqtfc"
      },
      "source": [
        "## Outline\n",
        "\n",
        "- [Introduction](#intro)\n",
        "- [Part 0: Installing and importing Python Packages](#0)\n",
        "- [Part 1: The SQUAD Question Answering Dataset](#1)\n",
        "    - [1.1 Extracting passages from SQUAD (Exercise 01)](#ex01)\n",
        "- [Part 2: Part 2: BM25 Passage Retrieval](#2)\n",
        "    - [2.1 Build index of passages using BM25](#2.1)\n",
        "    - [2.2 Computing document similarity and retrieving best documents](#2.2)\n",
        "    - [2.3 Evaluation using Top-K retrieval accuracy (Exercise 02)](#ex02)\n",
        "- [Part 3: BERT-based Passage Retrieval](#3)\n",
        "    - [3.1 Loading the BERT model](#3.1)\n",
        "    - [3.2 Computing text embeddings using BERT (Exercise 03)](#ex03)\n",
        "    - [3.3 Computing document similarity with query (Exercise 04)](#ex04)\n",
        "    - [3.4 Evaluation using Top-K retrieval accuracy](#3.4)\n",
        "- [Part 4: Dense Passage Retrieval (DPR)](#4)\n",
        "    - [4.1 Computing question and passage embeddings using DPR](#4.1)\n",
        "    - [4.2 Evaluation using Top-K retrieval accuracy](#4.2)\n",
        "    - [4.3 BM25 vs BERT vs DPR (Homework Exercise)](#hwex01)\n",
        "- [Part 5: Question Answering using DPR Reader](#5)\n",
        "- [Part 6 : End-to-end Open Domain QA on SQuAD using DPR Retriver-Reader](#6)\n",
        "    - [6.1 Retrieving top-k documents using DPR Retriever (Exercise 05)](#ex05)\n",
        "    - [6.2 End-to-end QA evaluation : Reading Comprehension vs Open-Domain QA (Exercise 06)](#ex06)\n",
        "    - [6.3 Open-domain QA evaluation : BM25 vs BERT vs DPR](#hwex02)\n",
        "- [Part 7: End-to-end Open Domain QA on SQuAD using DPR Retriver and GPT-3](#7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQi-jjy9qtfd"
      },
      "source": [
        "\n",
        "<a name='intro'></a>\n",
        "# Introduction\n",
        "\n",
        "In this tutorial you will explore various models for **passage retrieval** and **question answering**. For passage retrieval, we will experiment with three models you studied in the presentation\n",
        " - BM25\n",
        " - BERT Average embeddings\n",
        " - Dense Passage Retreival\n",
        "\n",
        "For Question answering (QA), we focus on  *reading comprehension* and *open-domain* forms of *Extractive QA* (where answer span is a substring of the passage).\n",
        " - Reading Comprehension : The passage for question is already given, and the model needs to search for answer in that passage\n",
        " - Open-Domain QA : The model first retrieves relevant passage(s) for given question, and then searches for answer in those passages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vekpd1Wkqtfe"
      },
      "source": [
        "<a name='0'></a>\n",
        "# Part 0: Installing and importing Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToJP6Mrwqtff"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install rank-bm25\n",
        "!pip install openai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T6G3om3qtfg"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOsmvPFQqtfh"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Part 1: The SQUAD Question Answering Dataset\n",
        "\n",
        "We will be using the [SQUAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset for QA experiments. It consists of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage.\n",
        "\n",
        "**Dataset details**\n",
        " - The dataset is available for download at https://rajpurkar.github.io/SQuAD-explorer/\n",
        " - There are 87,599, 10,570, and 10,821 rows of data in train, validation and test sets respectively.\n",
        " - We will be just using train and dev datasets as test dataset labels are hidden. Users can submit their models on test set by following instructions at https://rajpurkar.github.io/SQuAD-explorer/\n",
        " - A data point looks like\n",
        "\n",
        "     {\n",
        "        \"answers\": {\n",
        "            \"answer_start\": [1],\n",
        "            \"text\": [\"This is a test text\"]\n",
        "        },\n",
        "        \"context\": \"This is a test context.\",\n",
        "        \"id\": \"1\",\n",
        "        \"question\": \"Is this a test?\",\n",
        "        \"title\": \"train test\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-5Q-Ls7qtfi"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "dataset['train'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhJVbrsdqtfk"
      },
      "source": [
        "# Since the dataset is huge, we will be using only a prt of time for our experiments\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(5000))\n",
        "dataset[\"validation\"] = dataset[\"validation\"].select(range(500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7m3oCB7rJRN"
      },
      "source": [
        "<a name='ex01'></a>\n",
        "### Exercise 01: Extracting passages from SQUAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QKYv5VEqtfm"
      },
      "source": [
        "def extract_passages(dataset) :\n",
        "\n",
        "    \"\"\"Returns a list of unique passages extracted from the dataset\n",
        "    NOTE: Two or more examples in a dataset can have same passage.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset) : Input dataset same as the SQUAD Dataset format\n",
        "\n",
        "    Returns:\n",
        "        List of passages in dataset\n",
        "    \"\"\"\n",
        "\n",
        "    corpus = [example['context'] for example in dataset]\n",
        "    corpus = list(set(corpus))\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjGL7oIsqtfm"
      },
      "source": [
        "train_corpus = extract_passages(dataset['train'])\n",
        "dev_corpus = extract_passages(dataset['validation'])\n",
        "full_corpus = train_corpus + dev_corpus\n",
        "\n",
        "tokenized_full_corpus = [doc.split(\" \") for doc in full_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAFRoOI9qtfn"
      },
      "source": [
        "<a name='2'></a>\n",
        "# Part 2: BM25 Passage Retrieval\n",
        "\n",
        "As explained in the talk, BM25 is based on TF-IDF which looks into two main factors to determine document's similarity with query\n",
        " - Term Frequency aka (TF): how often do the query terms occur in the document?\n",
        " - Inverse Document Frequency (IDF): how many documents the term appeared in?\n",
        "\n",
        "For this tutorial, we will use the [rank_bm25 API](https://pypi.org/project/rank-bm25/) to first build indexes of passages, and then retrieve documents given a query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdJ35fLqtfo"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### Build index of passages using BM25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uynli-slqtfo"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "bm25 = BM25Okapi(tokenized_full_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqhJMPXqtfp"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### Computing document similarity and retrieving best documents\n",
        "\n",
        "BM25 library provides following functions:\n",
        " - `get_scores(query)` function to get query similarity with all documents\n",
        " - `get_top_n(query, n)` directly n documents having highest similarity score with given query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUKzgRt9qtfp"
      },
      "source": [
        "query = \"Who is Beyoncé Giselle Knowles-Carter?\"\n",
        "doc_scores = bm25.get_scores(query.split(' '))\n",
        "print ('Document scores length of first 5 documents : ', doc_scores[:5])\n",
        "assert len(doc_scores) == len(full_corpus)\n",
        "top_passages = bm25.get_top_n(query.split(\" \"), full_corpus, n=3)\n",
        "for p in top_passages :\n",
        "    print('-----------------------------------------------------')\n",
        "    print (p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfPjCek3qtfq"
      },
      "source": [
        "<a name='ex02'></a>\n",
        "### Exercise 02: Evaluation using Top-K retrieval accuracy\n",
        "\n",
        "The top-k accuracy for a retrieval model is defined as fraction of questions for which correct passage appears in top-k retrieved passages.\n",
        "\n",
        "So accuracy-wise `top-1 < top-10 < top-50 < top-100`\n",
        "\n",
        "Complete below function `get_doc_rank` which computes ranking of the document for given query. If the document has highest bm25 score, the rank will be 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzXFf6ioqtfq"
      },
      "source": [
        "def get_doc_rank(passage, passages, doc_scores) :\n",
        "    \"\"\"\n",
        "    Returns ranking of document for given query.\n",
        "    Args:\n",
        "        - passage (str) : the document for which rank is to be computed\n",
        "        - passages (list[str]) : all documents indexed in BM25\n",
        "        - doc_scores (list[int]) : BM25 scores of all documents\n",
        "    Returns:\n",
        "        - doc_rank (int) : rank of document between 1 and len(passages)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_rank = 1\n",
        "    index = passages.index(passage)\n",
        "    for i in range(len(passages)) :\n",
        "        if index!=i and doc_scores[i] > doc_scores[index] :\n",
        "            doc_rank += 1\n",
        "\n",
        "    return doc_rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsjtCTf4qtfr"
      },
      "source": [
        "top_k = [1,5,10,20,40,50,100]\n",
        "correct_cnt_bm25 = {k:0 for k in top_k}\n",
        "for example in dataset['validation'] :\n",
        "\n",
        "    #Get scores of all documents for given query\n",
        "    doc_scores = bm25.get_scores(example['question'].split(' '))\n",
        "\n",
        "    #Get rank of ground truth document\n",
        "    doc_rank = get_doc_rank(example['context'], full_corpus, doc_scores)\n",
        "\n",
        "    #Update top_k counts correctly\n",
        "    for k in top_k :\n",
        "        if doc_rank <= k :\n",
        "            correct_cnt_bm25[k] += 1\n",
        "\n",
        "for k in top_k :\n",
        "    print ('Top-{} accuracy BM25: {}%'.format(k, 100*correct_cnt_bm25[k]/len(dataset['validation'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P144AAXbdexP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkK2UW18qtfr"
      },
      "source": [
        "<a name='3'></a>\n",
        "# Part 3: BERT-based Passage Retrieval\n",
        "\n",
        "Now we will see how to leverage BERT for passage retrieval. We will use BERT embeddings to represent passages and queries.\n",
        "\n",
        "The workflow will look like-\n",
        " 1. Compute BERT embeddings of all passages in the corpus\n",
        " 2. Compute BERT embeddings of query\n",
        " 3. Compute score of each passage for query by simply taking dot product of query vector and passage vector\n",
        " 4. Return top-k passages with highest scores\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=15qXd_-Cu4LpIWoVr10tLdd12KYPCGS0j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESM9m542qtfr"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### Loading the BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "yxftHWzbqtfr"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "bert_model.to(device)\n",
        "\n",
        "#Set mode to eval since we are not training/fine-tuning BERT weights\n",
        "bert_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69kpoymkqtfs"
      },
      "source": [
        "Run the cell below to see how to get BERT word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIAX9G7Lqtfs"
      },
      "source": [
        "encoded_input = bert_tokenizer(['This is a dummy passage1', 'This is a dummy passage2'],\n",
        "                               padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    model_output = bert_model(**encoded_input)\n",
        "    token_embeddings = model_output[0]\n",
        "token_embeddings.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIYcwJbgqtft"
      },
      "source": [
        "<a name='ex03a'></a>\n",
        "### Exercise 03a: Computing sentence embeddings using BERT\n",
        "\n",
        "We explore two methods to compute sentence embeddings of passages and queries:\n",
        " - Average word embeddings : Encoding sentence with BERT, then computing the average of output word embeddings of text.\n",
        " - [CLS] token embeddings : As we learnt in previous tutorial, the embedding of CLS token can be used as a sentence representation.\n",
        "\n",
        "Complete below function `compute_bert_avg_embeddings` which computes text embeddings by encoding sentence with BERT, then computing the average of output word embeddings of text. Refer to https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b on how to extract word embeddings from BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkSYnGrBqtft"
      },
      "source": [
        "def compute_bert_avg_embeddings(sentences) :\n",
        "\n",
        "    \"\"\"\n",
        "    Returns average word embedding for given query.\n",
        "    Args:\n",
        "        - sentences (list[str]) : list of sentences\n",
        "    Returns:\n",
        "        - embeddings : numpy array of size `len(sentences) X 768`\n",
        "    \"\"\"\n",
        "\n",
        "    encoded_input = bert_tokenizer(sentences,\n",
        "                               padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        model_output = bert_model(**encoded_input)\n",
        "    token_embeddings = model_output[0]\n",
        "    return torch.mean(token_embeddings, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL85IxmnOYDE"
      },
      "source": [
        "Complete below function `compute_bert_cls_embeddings` which computes text embeddings by encoding sentence with BERT, then returning the embedding of [CLS] token. Refer to https://huggingface.co/sentence-transformers/bert-base-nli-cls-token on how to extract [CLS] token embeddings from BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrE9syNKOgik"
      },
      "source": [
        "def compute_bert_cls_embeddings(sentences) :\n",
        "\n",
        "    \"\"\"\n",
        "    Returns [CLS] token BERT embedding for given query.\n",
        "    Args:\n",
        "        - sentences (list[str]) : list of sentences\n",
        "    Returns:\n",
        "        - embeddings : numpy array of size `len(sentences) X 768`\n",
        "    \"\"\"\n",
        "\n",
        "    encoded_input = bert_tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        model_output = bert_model(**encoded_input)\n",
        "    return model_output[0][:,0].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu8uVaCSqtft"
      },
      "source": [
        "The below commented code computes BERT average embeddings for all passages in full corpus. This will take ~2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "8gVLYBTtqtft"
      },
      "source": [
        "bert_avg_embeddings = torch.zeros([len(full_corpus), 768])\n",
        "batch_size = 3\n",
        "n_iters = len(full_corpus)//batch_size\n",
        "for i in range(n_iters) :\n",
        "    if i % 100 == 0:\n",
        "      print(i)\n",
        "    bert_avg_embeddings[i*batch_size:(i+1)*batch_size] = compute_bert_avg_embeddings(full_corpus[i*batch_size:(i+1)*batch_size])\n",
        "\n",
        "assert len(bert_avg_embeddings) == len(full_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now do the same thing for BERT CLS embeddings."
      ],
      "metadata": {
        "id": "Ide6Y__gbfRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_cls_embeddings = torch.zeros([len(full_corpus), 768])\n",
        "batch_size = 3\n",
        "n_iters = len(full_corpus)//batch_size\n",
        "for i in range(n_iters) :\n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "    bert_cls_embeddings[i*batch_size:(i+1)*batch_size] = compute_bert_cls_embeddings(full_corpus[i*batch_size:(i+1)*batch_size])\n"
      ],
      "metadata": {
        "id": "8SKid_y5bbgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COLp8DQHqtfu"
      },
      "source": [
        "<a name='ex04'></a>\n",
        "### Exercise 04: Computing document similarity with query\n",
        "\n",
        "Complete below function `compute_doc_scores` which computes cosine similarity between documents and query vectors.\n",
        "\n",
        "Use the `compute_bert_avg_embeddings` function to compute passage and query embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBdTLqYqqtfv"
      },
      "source": [
        "from torch import nn\n",
        "def compute_doc_scores(query, embeddings) :\n",
        "    \"\"\"\n",
        "    Returns similarity of documents with query\n",
        "    Args:\n",
        "        - query (str) : the question (query)\n",
        "        - embeddings (numpy array) : embeddings of documents\n",
        "    Returns:\n",
        "        - doc_scores : numpy array of size `len(sentences)`\n",
        "    \"\"\"\n",
        "\n",
        "    query_embedding = compute_bert_avg_embeddings([query])[0]\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    return cos(embeddings.to(device), query_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtVkN7MTqtfv"
      },
      "source": [
        "<a name='3.4'></a>\n",
        "### Evaluation using Top-K retrieval accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYAg_yPYqtfv"
      },
      "source": [
        "top_k = [1,5,10,20,40,50,100]\n",
        "correct_cnt_bert = {k:0 for k in top_k}\n",
        "\n",
        "for example in dataset['validation'] :\n",
        "    doc_scores = compute_doc_scores(example['question'], bert_avg_embeddings)\n",
        "    doc_rank = get_doc_rank(example['context'], full_corpus, doc_scores)\n",
        "    for k in top_k :\n",
        "        if doc_rank <= k :\n",
        "            correct_cnt_bert[k] += 1\n",
        "\n",
        "for k in top_k :\n",
        "    print ('Top-{} accuracy BERT Embeddings: {}%'.format(k, 100*correct_cnt_bert[k]/len(dataset['validation'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3eOhqpfqtfv"
      },
      "source": [
        "<a name='4'></a>\n",
        "# Part 4: Dense Passage Retrieval (DPR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UXtlCtcqtfw"
      },
      "source": [
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "q_model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "p_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "p_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "\n",
        "q_model.to(device)\n",
        "p_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_T9C4DTqtfw"
      },
      "source": [
        "<a name='4.1'></a>\n",
        "### Computing question and passage embeddings using DPR\n",
        "\n",
        "Run the example below to see how to compute embeddings for a question and passage using DPR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujo_wlDhqtfw"
      },
      "source": [
        "p_input_ids = p_tokenizer(['This is a dummy passage'],\n",
        "                        return_tensors='pt', padding=True, truncation=True)[\"input_ids\"].to(device)\n",
        "passage_embeddings = p_model(p_input_ids).pooler_output\n",
        "\n",
        "q_input_ids = q_tokenizer(['Is this a dummy question?'],\n",
        "                        return_tensors='pt', padding=True, truncation=True)[\"input_ids\"].to(device)\n",
        "question_embeddings = q_model(q_input_ids).pooler_output\n",
        "\n",
        "print ('Passage embedding size : ', passage_embeddings.size())\n",
        "print ('Question embedding size : ', question_embeddings.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Dfm_s6qtfw"
      },
      "source": [
        "We will now compute DPR embeddings for the entire corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "swXOy8tnqtfx"
      },
      "source": [
        "dpr_embeddings = torch.zeros([len(full_corpus), 768])\n",
        "\n",
        "for i in range(len(full_corpus)) :\n",
        "    if i%100==0 :\n",
        "        print (i)\n",
        "    with torch.no_grad():\n",
        "        input_ids = p_tokenizer(full_corpus[i*1:(i+1)*1],\n",
        "                        return_tensors='pt', padding=True, truncation=True)[\"input_ids\"].to(device)\n",
        "        dpr_embeddings[i*1:(i+1)*1] = p_model(input_ids).pooler_output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "dpr_embeddings = dpr_embeddings.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The query-document similarity in DPR is defined by inner product."
      ],
      "metadata": {
        "id": "x2ngEltQdqET"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cQsh2uUqtfx"
      },
      "source": [
        "def compute_doc_scores(query, embeddings) :\n",
        "    input_ids = q_tokenizer(query,\n",
        "                        return_tensors='pt', padding=True, truncation=True)[\"input_ids\"].to(device)\n",
        "    q_embeddings = q_model(input_ids).pooler_output\n",
        "\n",
        "    return torch.matmul(embeddings, q_embeddings.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PKxMx2Aqtfx"
      },
      "source": [
        "<a name='4.2'></a>\n",
        "### Evaluation using Top-K retrieval accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PRNPdg4qtfx"
      },
      "source": [
        "top_k = [1,5,10,20,40,50,100]\n",
        "correct_cnt_dpr = {k:0 for k in top_k}\n",
        "\n",
        "for example in dataset['validation'] :\n",
        "    doc_scores = compute_doc_scores(example['question'], dpr_embeddings)\n",
        "    doc_rank = get_doc_rank(example['context'], full_corpus, doc_scores)\n",
        "    for k in top_k :\n",
        "        if doc_rank <= k :\n",
        "            correct_cnt_dpr[k] += 1\n",
        "\n",
        "for k in top_k :\n",
        "    print ('Top-{} accuracy DPR Embeddings: {}%'.format(k, 100*correct_cnt_dpr[k]/len(dataset['validation'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1mNmpxrqtfy"
      },
      "source": [
        "<a name='hwex01'></a>\n",
        "### Homework Exercise 01: BM25 vs BERT vs DPR\n",
        "\n",
        "Now that you have top-k accuracy results of all three models, complete the below function `plot_histogram` to plot histograms comparing the  accuracy of BM25, BERT, and DPR in same plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JUTlds8qtfy"
      },
      "source": [
        "def plot_histogram(correct_cnt_bm25, correct_cnt_bert, correct_cnt_dpr) :\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'pass' WITH YOUR CODE) ###\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzdPMosYqtfy"
      },
      "source": [
        "<a name='5'></a>\n",
        "# Part 5 : Question Answering using DPR Reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W9rp8Vaqtfy"
      },
      "source": [
        "Study the below example to see how to perform Reading Comprehension using DPR Reader.\n",
        "The reader takes as input the query question, set of documents and performs extractive QA to extract the answer from set of documents.\n",
        "NOTE : We do not use the retriever in Reading Comprehension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fFFBEUrqtfy"
      },
      "source": [
        "from transformers import DPRReader, DPRReaderTokenizer\n",
        "reader_tokenizer = DPRReaderTokenizer.from_pretrained('facebook/dpr-reader-single-nq-base')\n",
        "reader_model = DPRReader.from_pretrained('facebook/dpr-reader-single-nq-base')\n",
        "\n",
        "reader_model.to(device)\n",
        "\n",
        "def dpr_reader_get_answer(questions, passages):\n",
        "\n",
        "  encoded_inputs = reader_tokenizer(\n",
        "      questions=questions,\n",
        "      texts=passages,\n",
        "      return_tensors='pt'\n",
        "  ).to(device)\n",
        "\n",
        "  input_ids = encoded_inputs[\"input_ids\"].tolist()[0]\n",
        "  outputs = reader_model(**encoded_inputs)\n",
        "  answer_start = torch.argmax(outputs.start_logits)\n",
        "  answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "  answer = reader_tokenizer.convert_tokens_to_string(reader_tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "  return answer\n",
        "\n",
        "answer = dpr_reader_get_answer([\"What is the capital of France?\"], [\"France is a unitary semi-presidential republic with its capital in Paris, the country's largest city and main cultural and commercial centre\"])\n",
        "print(f\"Question: What is the capital of France?\")\n",
        "print(f\"Answer: {answer}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYVpa05Tqtfz"
      },
      "source": [
        "<a name='6'></a>\n",
        "# Part 6 : End-to-end Open Domain QA on SQuAD using DPR Retriver-Reader\n",
        "\n",
        "In the final stage, we will perform open domain QA. This means first retriever will retrieve k documents for given query. Then reader will perform extractive QA and return the predicted answer span from one of the k documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAOV6OYLqtfz"
      },
      "source": [
        "<a name='ex05'></a>\n",
        "### Exercise 05: Retrieving top-k documents using DPR Retriever\n",
        "\n",
        "Complete below function `retrieve_top_k` which retrieves top-k documents using DPR retriever"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXLmXkDjqtfz"
      },
      "source": [
        "def retrieve_top_k(query, k) :\n",
        "    \"\"\"\n",
        "    Returns list of k passages having highest dot product with query vector\n",
        "    Args:\n",
        "        - query (str) : the question (query)\n",
        "        - k (int) : number of documents to be returned\n",
        "    Returns:\n",
        "        - passages (list[str]) : numpy array of size `len(sentences)`\n",
        "    \"\"\"\n",
        "\n",
        "    doc_scores = compute_doc_scores(query, dpr_embeddings).detach().cpu().numpy()[:,0]\n",
        "    max_indexes = doc_scores.argsort()[-k:]\n",
        "    passages = [full_corpus[ind] for ind in max_indexes]\n",
        "    return passages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS2gKhp8qtfz"
      },
      "source": [
        "# these functions are heavily influenced by the HF squad_metrics.py script\n",
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBzJP7LZqtfz"
      },
      "source": [
        "<a name='ex06'></a>\n",
        "### Exercise 06: End-to-end QA evaluation : Reading Comprehension vs Open-Domain QA\n",
        "\n",
        "Complete parts of below code to perform end-to-end extractive QA, evaluating on SQUAD dataset.\n",
        "\n",
        "The goal is to evaluate both reading comprehension (where correct passage is given for reader) and end-to-end open-domain (where retriever fetches passage for reader) models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSdo9_Oqtf0"
      },
      "source": [
        "f1_score = 0\n",
        "\n",
        "# If `open_domain==False` this means reading comprehension\n",
        "open_domain = False\n",
        "\n",
        "for example in dataset['validation'] :\n",
        "\n",
        "    passages = None\n",
        "    if open_domain : # Retrieve k=1 documents using DPR retriever\n",
        "        passages = retrieve_top_k(example['question'], 1)\n",
        "    else : # Passages will be the ground truth passage in example\n",
        "        passages = [example['context']]\n",
        "\n",
        "    # Use DPR reader to compute answer string\n",
        "    prediction = dpr_reader_get_answer([example['question']], passages)\n",
        "\n",
        "    f1_score += max((compute_f1(prediction, answer)) for answer in example['answers']['text'])\n",
        "\n",
        "if open_domain :\n",
        "    print ('Open domain QA accuracy (%) : ', 100*f1_score/len(dataset['validation']))\n",
        "else :\n",
        "    print ('Reading Comprehension accuracy (%) : ', 100*f1_score/len(dataset['validation']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkqbd0Epqtf0"
      },
      "source": [
        "<a name='hwex02'></a>\n",
        "### Homework Exercise 02: Open-domain QA evaluation : BM25 vs BERT vs DPR\n",
        "Now that you have evaluated DPR on open-domain QA, try to evaluate using BM25 and BERT for retrieving documents for open-domain QA. Compare the f1 scores of all three models. Do you see similar trends as seen in retrieval performance ?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MpM_QFtuiIi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='7'></a>\n",
        "# Part 7 : End-to-end Open Domain QA on SQuAD using DPR Retriver and GPT-3"
      ],
      "metadata": {
        "id": "_CWv06nWiI0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial setup is similar to what we did for sentiment analysis"
      ],
      "metadata": {
        "id": "VR0ZziDjiW8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "177WvvN1iNcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "dbCnA-tDigRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first try closed book QA, i.e. GPT-3 will answer the question **without** using retrieved passages."
      ],
      "metadata": {
        "id": "4NgJW7OwnESr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'I am a highly intelligent question answering bot.\\\n",
        "If you ask me a question that is rooted in truth, I will give you the answer. \\\n",
        "If you ask me a question that is nonsense, trickery, or has no clear answer, \\\n",
        "I will respond with \"Unknown\".\\n\\nQ: {question}'"
      ],
      "metadata": {
        "id": "_rVe87qinCyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset['validation'][0]\n",
        "prompt = template.format(question=sample['question'])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "kw83rCOcnGUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us get response from GPT-3 and compare it with ground truth"
      ],
      "metadata": {
        "id": "9m4bDdvSndgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpt_response(prompt):\n",
        "  response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    max_tokens=60,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0.5,\n",
        "    presence_penalty=0\n",
        "  )\n",
        "  return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "print('GPT-3 Response (closed-book) - ' + get_gpt_response(prompt))\n",
        "print('Ground Truth answers : ' + ', '.join(sample['answers']['text']))"
      ],
      "metadata": {
        "id": "knSPn2NTnW_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iDrC-ADtkC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now try augmenting GPT-3 will retrieved passages."
      ],
      "metadata": {
        "id": "Ue3KWmRKnzsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original answer without any passage\n",
        "query = 'How many turnovers did Cam Newton have in Super Bowl 50?'\n",
        "print(get_gpt_response(template.format(question=query)))"
      ],
      "metadata": {
        "id": "GLLWX30Yn8Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us design a new tempate that considers the top retrieved passage from DPR\n"
      ],
      "metadata": {
        "id": "jSmudiQhsrMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'Please answer the following question given the following passages:\\n{retrieved_passages}\\nQuestion: {query}\\nAnswer:'"
      ],
      "metadata": {
        "id": "EzieJGyEoGTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
        "# Create the prompt using the template above for the given query\n",
        "passage = retrieve_top_k(query, 1)[0]\n",
        "prompt = template.format(retrieved_passage=passage, question=query)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "45PvZcJxs1pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see if providing the passage makes a difference!"
      ],
      "metadata": {
        "id": "S7fcT_WPtMpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPT-3 Response (closed-book) - ' + get_gpt_response(prompt))\n",
        "print('Ground Truth answers : ' + ', '.join(dataset['validation'][63]['answers']['text']))"
      ],
      "metadata": {
        "id": "QfUDPzNhtFRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yUEvVNis9im"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}